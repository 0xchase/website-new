---
title: 'The Interpretable Transformer'
date: "2025-03-02"
---

## Introduction

There are a variety of blog posts and articles explaining self-attention and the transformer architecture.

Write from a different perspective.

Use this as a companion to those other posts.

Substantially sourced from transformer-circuits.pub, but as an explanation instead of an argument.

Some research is speculative so this may be wrong.

I'll update this post.

## Linear Algebra (collapsed)

### Vectors and Matricies

What are they really.

### Addition and Multiplication

What *is* vector addition and multiplication.

### Dot Product (collapsed)

Visual review of what a dot product really is.

## Transformer

- Residual stream
- Self attention
  - Copying data
- Multi layer perceptron
  - As a key-value store
